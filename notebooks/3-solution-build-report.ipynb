{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text De-Toxification, part III: Building Solution\n",
    "### Robert Chen, B20-AI\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# Small crutch to make Jupyter see the source folder\n",
    "if not sys.path.count(str(Path(os.path.realpath(\"\")).parent)):\n",
    "    sys.path.append(str(Path(os.path.realpath(\"\")).parent))\n",
    "from src.models.condbert.chooser import SimilarityChooser\n",
    "from src.models.condbert.predictor import MaskedTokenPredictorBERT\n",
    "from src.models.condbert.rewriter import CondBERTRewriter\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import string\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [original paper](https://arxiv.org/pdf/2109.08914.pdf) uses two approaches for the *text detoxification* task: **condBERT** and **ParaGeDi**. **ParaGeDi** generates completely new text based on the input, while **condBERT** identifies every word with high toxicity score and substitutes it with less toxic synonyms. Both models performed far better than the rest of fine-tuned language models. We will use the **condBERT** for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set up the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we set up the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words, positive_words = [], []\n",
    "with open('../data/interim/condbert_vocab/negative_words.txt', 'r') as f:\n",
    "    tmp = f.readlines()\n",
    "negative_words = list(map(lambda x: x[:-1], tmp))\n",
    "with open('../data/interim/condbert_vocab/toxic_words.txt', 'r') as f:\n",
    "    tmp = f.readlines()\n",
    "negative_words += list(map(lambda x: x[:-1], tmp))\n",
    "with open('../data/interim/condbert_vocab/positive_words.txt', 'r') as f:\n",
    "    tmp = f.readlines()\n",
    "positive_words = list(map(lambda x: x[:-1], tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/interim/condbert_vocab/word2coef.pkl', 'rb') as f:\n",
    "    word2coef = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_toxicities = []\n",
    "with open('../data/interim/condbert_vocab/token_toxicities.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        token_toxicities.append(float(line))\n",
    "token_toxicities = np.array(token_toxicities)\n",
    "token_toxicities = np.maximum(0, np.log(1 / (1 / token_toxicities - 1)))\n",
    "\n",
    "for token in string.punctuation:\n",
    "    token_toxicities[tokenizer.encode(token)][1] = 3\n",
    "token_toxicities[tokenizer.encode('you')][1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assembling the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = MaskedTokenPredictorBERT(model, tokenizer, max_len=250, target=0, device=device)\n",
    "chooser = SimilarityChooser(coeff=10, tokenizer=tokenizer)\n",
    "rewriter = CondBERTRewriter(model, tokenizer, device, \n",
    "                            negative_words, positive_words, \n",
    "                            word2coef, token_toxicities, predictor)\n",
    "def adjust_logits(logits, target=0):\n",
    "    return logits - rewriter.tox_scores.cpu().numpy() * (1 - 2 * target) * 10\n",
    " \n",
    "predictor.logits_postprocessor = adjust_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are a wayward little snob .\n",
      "as hyperactive and hyperactive as his boss\n",
      "a non - nonpartisan society does odd little things and votes for non - partisan politicians .\n",
      "a president who is an anomaly .\n",
      "colbert is a real indignated young man , and is a hard working man .\n",
      "brave , brave , brave , brave , brave , brave , brave , brave , brave .\n",
      "i hate you , i think you are an honest man , and youre contributions are so marvellous .\n",
      "luna is a cute little , cute cutest girl with no brains , smells bad , is po , and is a pimple to the worlds ' very core !\n",
      "mormons are crazy mormons are crazy mormons are crazy mormons are crazy mormons are crazy mormons are crazy mormons are crazy mormons are crazy mormons are crazy mormons are crazy mormons !\n",
      "youre a very bad illogical , very bad illogical , busybody piece of douche .\n"
     ]
    }
   ],
   "source": [
    "with open('../data/interim/condbert_vocab/test/test_10k_toxic', 'r') as f:\n",
    "    lines = f.readlines()[:10]\n",
    "    for i, line in enumerate(lines):\n",
    "        x = line.strip()\n",
    "        y = rewriter.replace(x, verbose=False, chooser=chooser, n_top=10, n_tokens=(1, 2, 3), n_units=3)\n",
    "        print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Tuning the similarity coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some parameters of the model that can be tuned in order to achieve better substitution. The first argument we will be trying to tune is the similarity coefficient in the `SimilarityChooser`. Firstly, let us choose the small subset from the test set and analyze the effects of changing the parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this car is so gay that it wont give you rides if your mexican !\\n',\n",
       " 'fuck those gold digging bitchs .\\n']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "with open('../data/interim/condbert_vocab/test/test_10k_toxic', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "random.seed(42)\n",
    "subset = random.choices(lines, k=2)\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us iterate over some coefficient values and look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chooser.coeff=0:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "one of those gold digging puppies .\n",
      "===========================\n",
      "chooser.coeff=10:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "one of those gold digging puppies .\n",
      "===========================\n",
      "chooser.coeff=20:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "i knew all those gold digging invoice .\n",
      "===========================\n",
      "chooser.coeff=30:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "i knew all those gold digging invoice .\n",
      "===========================\n",
      "chooser.coeff=40:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "i knew all those gold digging invoice .\n",
      "===========================\n",
      "chooser.coeff=50:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "i knew all those gold digging invoice .\n",
      "===========================\n",
      "chooser.coeff=60:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "i knew all those gold digging invoice .\n",
      "===========================\n",
      "chooser.coeff=70:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "i knew all those gold digging invoice .\n",
      "===========================\n",
      "chooser.coeff=80:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "i never liked those gold diggings much .\n",
      "===========================\n",
      "chooser.coeff=90:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "i never liked those gold diggings much .\n",
      "===========================\n",
      "chooser.coeff=100:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "i never liked those gold diggings much .\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "coeffs = [t for t in range(0, 110, 10)]\n",
    "for i in coeffs:\n",
    "    chooser.coeff = i\n",
    "    results = [rewriter.replace(x, chooser=chooser, n_tokens=(1, 2, 3), n_top=10, n_units=3, verbose=False) for x in subset]\n",
    "    print(f'{chooser.coeff=}:')\n",
    "    print('\\n'.join(results))\n",
    "    print('===========================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The responses are still not very similar to the original statements. Most probably because of the coefficient in `adjust_logits` function which makes the model to avoid toxic words. Let us try fine-tuning this one too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Tuning the toxicity penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us iterate over the same subset with different toxicity penalties (resetting the similarity coefficient to default one too):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalty=0:\n",
      "this car is so gay that it wont give you rides if your mexican !\n",
      "one of those gold digging bitch .\n",
      "===========================\n",
      "penalty=1:\n",
      "this car is so gay that it wont give you rides if your mexican !\n",
      "one of those gold digging bitch .\n",
      "===========================\n",
      "penalty=2:\n",
      "this car is so gay that it wont give you rides if your mexican !\n",
      "one of those gold digging women .\n",
      "===========================\n",
      "penalty=3:\n",
      "this car is so gay that it wont give you rides if your mexican !\n",
      "one of those gold digging women .\n",
      "===========================\n",
      "penalty=4:\n",
      "this car is so gay that it wont give you rides if your mexican !\n",
      "one of those gold digging women .\n",
      "===========================\n",
      "penalty=5:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "one of those gold digging puppies .\n",
      "===========================\n",
      "penalty=6:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "one of those gold digging puppies .\n",
      "===========================\n",
      "penalty=7:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "one of those gold digging puppies .\n",
      "===========================\n",
      "penalty=8:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "one of those gold digging puppies .\n",
      "===========================\n",
      "penalty=9:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "one of those gold digging puppies .\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "penalty = 0\n",
    "chooser.coeff = 10\n",
    "\n",
    "def adjust_logits(logits, target=0):\n",
    "    global penalty\n",
    "    return logits - rewriter.tox_scores.cpu().numpy() * penalty\n",
    "\n",
    "for pen in range(0, 10):\n",
    "    penalty = pen\n",
    "    predictor.logits_postprocessor = adjust_logits\n",
    "    results = [rewriter.replace(x, chooser=chooser, n_tokens=(1, 2, 3), n_top=10, n_units=3, verbose=False) for x in subset]\n",
    "    print(f'{penalty=}:')\n",
    "    print('\\n'.join(results))\n",
    "    print('===========================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Creating more tokens, re-ranking more hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the parameters like `n_tokens` and `n_top` also affect the generation process. `n_tokens` sets the possible number of tokens that can be generated from one word, `n_top` sets the amount of BERT hypotheses that are re-ranked each time. Let us try to see the effect of changing these parameters and try to balance out all of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens_batch = [(1, ), (2, ), (3, ), (1, 2), (2, 3), (3, 4), (1, 2, 3, 4), (1, 2, 3, 4, 5)]\n",
    "n_top_batch = [t for t in range(5, 20, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens=(1,):\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "crud those gold digging witch .\n",
      "=================\n",
      "n_tokens=(2,):\n",
      "this car is so much bigger that it wont give you rides if you go out there !\n",
      "oops , all of those gold digging frolic days .\n",
      "=================\n",
      "n_tokens=(3,):\n",
      "this car is so high - tech that it wont give you rides if you are a good enough driver !\n",
      "i know all of them are gold digging douche gold digging .\n",
      "=================\n",
      "n_tokens=(1, 2):\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "oops all those gold diggings again .\n",
      "=================\n",
      "n_tokens=(2, 3):\n",
      "this car is so high - tech that it wont give you rides if you turn it on !\n",
      "one of those other gold diggings over there .\n",
      "=================\n",
      "n_tokens=(3, 4):\n",
      "this car is so unconcerned that it wont give you rides if you are a very good driver !\n",
      "i know all of them are gold digging douche gold digging .\n",
      "=================\n",
      "n_tokens=(1, 2, 3, 4):\n",
      "this car is so unconcerned that it wont give you rides if you want to !\n",
      "one of those gold diggings , anyways .\n",
      "=================\n",
      "n_tokens=(1, 2, 3, 4, 5):\n",
      "this car is so unconcerned , that it wont give you rides if you do that !\n",
      "oh my god all of those gold diggings were going to happen .\n",
      "=================\n"
     ]
    }
   ],
   "source": [
    "for n_tokens in n_tokens_batch:\n",
    "    results = [rewriter.replace(x, chooser=chooser, n_tokens=n_tokens, n_top=10, n_units=3, verbose=False) for x in subset]\n",
    "    print(f'{n_tokens=}:')\n",
    "    print('\\n'.join(results))\n",
    "    print('=================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_top=5:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "not like those gold diggings here .\n",
      "=================\n",
      "n_top=7:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "i knew all those gold digging invoice .\n",
      "=================\n",
      "n_top=9:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "one of those gold digging puppies .\n",
      "=================\n",
      "n_top=11:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "one of those gold digging puppies .\n",
      "=================\n",
      "n_top=13:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "one of those gold digging puppies .\n",
      "=================\n",
      "n_top=15:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "one of those gold digging puppies .\n",
      "=================\n",
      "n_top=17:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "one of those gold digging puppies .\n",
      "=================\n",
      "n_top=19:\n",
      "this car is so queer that it wont give you rides if your mexican !\n",
      "oops all those gold digging around here .\n",
      "=================\n"
     ]
    }
   ],
   "source": [
    "for n_top in n_top_batch:\n",
    "    results = [rewriter.replace(x, chooser=chooser, n_tokens=(1, 2, 3), n_top=n_top, n_units=3, verbose=False) for x in subset]\n",
    "    print(f'{n_top=}:')\n",
    "    print('\\n'.join(results))\n",
    "    print('=================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
